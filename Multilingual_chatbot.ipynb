{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ebhawana/multilingual-chatbot/blob/main/Multilingual_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQDx1ss_DIBq",
        "outputId": "66fed525-2133-43e6-f517-292f1ec8e118"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: langid in /usr/local/lib/python3.11/dist-packages (1.1.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers torch sentencepiece langid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYCcspH0Dc_a"
      },
      "outputs": [],
      "source": [
        "from langdetect import detect\n",
        "from transformers import (AutoTokenizer, AutoModelForSeq2SeqLM,AutoModelForCausalLM,\n",
        "                          XLMRobertaTokenizer, XLMRobertaForSequenceClassification,\n",
        "                          AutoModelForSequenceClassification,\n",
        "                          MBartForConditionalGeneration, MBart50TokenizerFast)\n",
        "import torch\n",
        "\n",
        "LANG_CODES = {\n",
        "    'fr': 'French',\n",
        "    'de': 'German',\n",
        "    'es': 'Spanish',\n",
        "    'hi': 'Hindi',\n",
        "    'en': 'English'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zGCeSR4raEV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "6fa2f5bf-1068-49c5-94c5-31cfc0de08ee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Load MarianMT models dynamically\\ndef get_translation_model(src_lang, tgt_lang):\\n    model_name = f\"Helsinki-NLP/opus-mt-{src_lang}-{tgt_lang}\"\\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\\n    return (tokenizer, model)\\n\\n\\n# Translate text\\ndef translate(text, tokenizer, model):\\n    inputs = tokenizer(text, return_tensors=\"pt\", padding=True,truncation=True)\\n    translated = model.generate(**inputs)\\n    return tokenizer.batch_decode(translated, skip_special_tokens=True)[0]\\n  \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "'''\n",
        "# Load MarianMT models dynamically\n",
        "def get_translation_model(src_lang, tgt_lang):\n",
        "    model_name = f\"Helsinki-NLP/opus-mt-{src_lang}-{tgt_lang}\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "    return (tokenizer, model)\n",
        "\n",
        "\n",
        "# Translate text\n",
        "def translate(text, tokenizer, model):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True,truncation=True)\n",
        "    translated = model.generate(**inputs)\n",
        "    return tokenizer.batch_decode(translated, skip_special_tokens=True)[0]\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MBART for translation\n",
        "mbart_model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
        "mbart_tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
        "lang_map = {\n",
        "    \"en\": \"en_XX\",\n",
        "    \"hi\": \"hi_IN\",\n",
        "    \"fr\": \"fr_XX\",\n",
        "    \"es\": \"es_XX\",\n",
        "    \"de\": \"de_DE\",\n",
        "    \"bn\": \"bn_IN\",\n",
        "    \"mr\": \"mr_IN\",\n",
        "    \"ta\": \"ta_IN\",\n",
        "    \"te\": \"te_IN\",\n",
        "}"
      ],
      "metadata": {
        "id": "9XYCq9fk9E5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(text, src_lang, tgt_lang):\n",
        "    mbart_tokenizer.src_lang = lang_map[src_lang]\n",
        "    encoded = mbart_tokenizer(text, return_tensors=\"pt\")\n",
        "    generated_tokens = mbart_model.generate(**encoded)\n",
        "    return mbart_tokenizer.decode(generated_tokens[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "zmwHEMhW9u5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Intent Classifier\n",
        "intent_tokenizer = AutoTokenizer.from_pretrained(\"Falconsai/intent_classification\")\n",
        "intent_model = AutoModelForSequenceClassification.from_pretrained(\"Falconsai/intent_classification\")\n",
        "intent_labels = {\n",
        "    0: \"weather\",\n",
        "    1: \"greeting\",\n",
        "    2: \"goodbye\",\n",
        "    3: \"thanks\",\n",
        "    4: \"joke\"\n",
        "}"
      ],
      "metadata": {
        "id": "-nE6zZGEjgr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_intent(text):\n",
        "    inputs = intent_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = intent_model(**inputs)\n",
        "        probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
        "        conf, pred = torch.max(probs, dim=1)\n",
        "    if conf.item() < 0.6:\n",
        "        return \"unknown\"\n",
        "    return intent_labels.get(pred.item(), \"unknown\")\n"
      ],
      "metadata": {
        "id": "pXg5xEOTjguB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVhfJL01rbUu"
      },
      "outputs": [],
      "source": [
        "# DialoGPT for dialogue\n",
        "dialogue_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
        "dialogue_model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brhj9fdSrry-"
      },
      "outputs": [],
      "source": [
        "# Chat history memory\n",
        "chat_history_ids = None\n",
        "\n",
        "def generate_dialogue_response(user_input_en):\n",
        "    global chat_history_ids\n",
        "    new_input_ids = dialogue_tokenizer.encode(user_input_en + dialogue_tokenizer.eos_token, return_tensors='pt')\n",
        "\n",
        "    bot_input_ids = torch.cat([chat_history_ids, new_input_ids], dim=-1) if chat_history_ids is not None else new_input_ids\n",
        "    chat_history_ids = dialogue_model.generate(bot_input_ids, max_length=2000, pad_token_id=dialogue_tokenizer.eos_token_id)\n",
        "\n",
        "    response = dialogue_tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "ypz_tytDCThH",
        "outputId": "4d1d74db-be7d-4f38-9a2a-01b9f7d65ead"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'responses = {\\n    \"hello\": \"Hello! How can I help you?\",\\n    \"how are you\": \"I\\'m just a bot, but I\\'m doing fine!\",\\n    \"what is your name\": \"I\\'m a multilingual chatbot.\",\\n    \"bye\": \"Goodbye! Have a great day!\",\\n    \"default\": \"I\\'m not sure how to respond to that.\",\\n    \"thanks\": \"You\\'re welcome!\",\\n    \"sorry\": \"It\\'s okay.\",\\n    }\\n\\n# Response generator\\ndef get_response(user_input):\\n    user_input = user_input.lower()\\n    for key in responses:\\n        if key in user_input:\\n            return responses[key]\\n    return \"Sorry, I didn’t understand that.\"\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "# Rule-based chatbot responses in English\n",
        "'''responses = {\n",
        "    \"hello\": \"Hello! How can I help you?\",\n",
        "    \"how are you\": \"I'm just a bot, but I'm doing fine!\",\n",
        "    \"what is your name\": \"I'm a multilingual chatbot.\",\n",
        "    \"bye\": \"Goodbye! Have a great day!\",\n",
        "    \"default\": \"I'm not sure how to respond to that.\",\n",
        "    \"thanks\": \"You're welcome!\",\n",
        "    \"sorry\": \"It's okay.\",\n",
        "    }\n",
        "\n",
        "# Response generator\n",
        "def get_response(user_input):\n",
        "    user_input = user_input.lower()\n",
        "    for key in responses:\n",
        "        if key in user_input:\n",
        "            return responses[key]\n",
        "    return \"Sorry, I didn’t understand that.\"\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import langid\n",
        "\n",
        "def detect_language(text):\n",
        "    lang, _ = langid.classify(text)\n",
        "    return lang"
      ],
      "metadata": {
        "id": "Nqm2yDQ03cvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfGg27bZCfNo"
      },
      "outputs": [],
      "source": [
        "# Chatbot pipeline\n",
        "def multilingual_chatbot(user_input):\n",
        "    detected_lang = detect_language(user_input)\n",
        "    print(\"Detected language: \",detected_lang)\n",
        "    if detected_lang not in lang_map:\n",
        "        return \"Sorry, I don't support this language yet.\"\n",
        "\n",
        "    #translate first to english\n",
        "    if detected_lang != \"en\":\n",
        "        #tokenizer, model = get_translation_model(detected_lang, \"en\")\n",
        "        input_en = translate(user_input, detected_lang, \"en\")\n",
        "    else:\n",
        "        input_en = user_input\n",
        "\n",
        "    #Classify intent\n",
        "    intent = classify_intent(input_en)\n",
        "    print(\"Intent: \",intent)\n",
        "\n",
        "     #Generate response in English\n",
        "    #prompt = f\"As a friendly assistant, answer this {intent} message: {input_en}\"\n",
        "    #response_en = generate_dialogue_response(prompt)\n",
        "\n",
        "\n",
        "    response_en = generate_dialogue_response(input_en)\n",
        "    print(\"Response in English: \",input_en)\n",
        "\n",
        "\n",
        "    #Translate back to english\n",
        "    if detected_lang != \"en\":\n",
        "        #tokenizer_back, model_back = get_translation_model(\"en\", detected_lang)\n",
        "        response_final = translate(response_en, detected_lang, \"en\")\n",
        "    else:\n",
        "        response_final = response_en\n",
        "\n",
        "    return response_final\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wP3qrPT_C80m",
        "outputId": "6e1ce48d-fdd3-46be-bd77-9d85dfba9aaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Multilingual Chatbot\n",
            "\n",
            "Hi I am MBot.(Type 'quit' to exit)\n",
            "\n",
            "You: आप कैसे हैं\n",
            "Detected language:  hi\n",
            "Intent:  unknown\n",
            "Response in English:  How are you\n",
            "Bot: I 'm good, how are you?\n",
            "\n",
            "You: Quit\n"
          ]
        }
      ],
      "source": [
        "# Example interaction\n",
        "print(\"Multilingual Chatbot\\n\")\n",
        "print(\"Hi I am MBot.(Type 'quit' to exit)\")\n",
        "while True:\n",
        "  user_input = input(\"\\nYou: \")\n",
        "  if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "    break\n",
        "  response = multilingual_chatbot(user_input)\n",
        "  print(\"Bot:\", response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J1gdlezH5hsU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJoxKg7MnBkQbmREE9zq6n",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}